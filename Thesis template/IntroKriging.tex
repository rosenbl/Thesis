
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace} 
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{natbib}
\geometry{a4paper} 
\renewcommand{\bibname}{Bibliography}


\title{Kriging}
\author{Blake Rosenthal}
\date{}


\begin{document}

\maketitle
\pagestyle{empty}

\section{Spatial statistics}
Kriging is a method utilized in the field of geostatistics to model spatial data. Given a spatially continuous random process $Y(x)$ over some two-dimensional region $B$, a sample $S_i: i=1, \dots, n$ is obtained from $Y$ at locations $x_i: i=1, \dots, n$. Since $S$ is only a small and incomplete realization of the field $Y$, the standard geostatistical approach is to impose an underlying structure to the field consisting of a mean function $\mu(s)$ and a random error process with zero mean $e(\mathbf{s})$. Together these specify that 

\begin{align*}
Y(\mathbf{s}) = \mu(\mathbf{s}) + e(\mathbf{s}),
\end{align*}

where $\mu(\mathbf{s}) = E[Y(\mathbf{s})]$.
  \\

The goal is to make some predictions regarding the underlying random process $Y$. Letting $T = \int_BY(x)dx$ allows us to interpolate over the sample region using the data $S_i$. Kriging at its simplest is a matter of predicting a value of $Y(x_i)$ at an arbitrary point within the region $B$. \emph{Simple kriging} assumes $Y$ to have a constant mean which is estimated from the sample mean of $S$. The predictor of $T$ is then the integral of the best linear predictor of $Y(x)$. \emph{Ordinary kriging} uses the estimated covariance structure of $Y$ to replace the sample mean with the generalized least squares estimate of $\mu$. Finally, \emph{universal kriging} uses a regression model for the mean. 

\section{Covariance and the variogram}
Part of the effectiveness of the kriging method comes from the recognition that the data from a spatial sample are correlated based proximity. Points closer together are expected to be more highly correlated than points with greater spatial separation. The variogram, or semivariogram, plots this correlation as a function of distance, and the empirical semivariogram is the known covariance structure of the data. The distance between any two points $x_i$ and $x_j$ can be described as a vector $\bold{x}$. For such a vector, the semivariogram of the stationary process $e$ can be described by $\gamma(\bold{x}) = {1\over 2} \text{var} \{ e(\bold{x}) - e(\bold{0}) \}$. An unbiased estimator of $\gamma(\bold{x})$ is

\begin{align*} 
   \hat\gamma(\bold{x}) = { 1 \over {2n(\bold{x})}} \sum_{\bold{x}_i - \bold{x}_j = x} \{ e(\bold{x}_i) - \
   e(\bold{x}_j) \} ^2
\end{align*}

\noindent where $n(\bold{x})$ is the number of pairs of points whose difference is within a specified tolerance of $\bold{x}$. This assumes that covariance between data points is a function of spatial distance only, and not location or other factors. This estimation also requires a subjective choice in "binning" -- since $\mathbf{x}$ represents an exact distance, or lag, between two points, it is necessary to group distances into representative intervals, or bins. A common way to do this is to make this binning choice up front, perhaps grouping the data into thirty or so bins then choosing $\mathbf{x}$ to be the average of all the lags that fall into a given bin. Therefore, unless the data is taken on a rectangular or polar grid, the accuracy of the semivariogram will always be dependent on the binning choices. What's the right number of bins? Well, there's a trade-off -- more bins means that $\mathbf{x}$ is a better estimation of its representative bin, yet there are fewer lags to any particular bin and therefore a greater sampling variation. This is an interesting optimization problem on its own, but the data itself may impose binning restrictions depending on the sample size and other factors. This means that there is therefore no uniquely optimized semivariogram. \\

Fitting a parametric model to the empirical variogram gives a convenient equation to work with for several reasons -- first, the empirical semivariogram is often all over the place, and a smoothed version will have a lower variance. Second, the empirical semivariogram usually fails to be conditionally nonpositive definite. This is a necessary condition when choosing predictors at later stages since the prediction error variance must be nonnegative at every point in the field. Third, predicting locations at lags not represented by the chosen bins requires a continuous function, something only a smoothed variogram can accomplish. This smoothed version must satisfy the following necessary and sufficient conditions to be a valid semivariogram: \\

\begin{enumerate}

\item Vanishing at 0: $\gamma(\bold{0}) = 0$
\item Evenness: $\gamma(-\bold{x}) = \gamma(\bold{x}$)
\item Conditional negative definiteness: $\sum_{i=1}^n \sum_{j=1}^n a_i a_j \gamma(x_i - x_j) \leq 0$ for all $n$, all $s_1, \dots, s_n$ and all $a_1, \dots, a_n$ such that $\sum_{i=1}^n a_i = 0$

\end{enumerate}

\section{Estimation of the mean function}

Simple, ordinary, and universal kriging all differ in their approach to estimating the mean function. Simple kriging, which assumes a constant mean, is typically dismissed by most statisticians since it usually fails to accurately describe any naturally occurring random process. Here we'll go over universal kriging since it is the best linear unbiased prediction model for geostatistical random fields. \\

The mean function is given by the linear equation

\begin{align*}
\mu(\mathbf{s}; \mathbf{\beta}) = \mathbf{X}(\mathbf{s})^T \mathbf{\beta}
\end{align*} 

where $\mathbf{X}(\mathbf{s})$ is a vector of covariates observed at $\mathbf{s}$. These variables could be simply latitude and longitude coordinates, but may also include such information as elevation, slope, windspeed, etc. If using only latitude and longitude, for example, a first order  trend surface model is given by

\begin{align*}
\mu(\mathbf{s}; \mathbf{\beta}) = \beta_0 + \beta_1s_1 + \beta_2s_2
\end{align*} 

where $\mathbf{s} = (s_1, s_2)$ are latitude and longitude. This can also be done with a second order equation

\begin{align*}
\mu(\mathbf{s}; \mathbf{\beta}) = \beta_0 + \beta_1s_1 + \beta_2s_2 + \beta_{11}s_1^2 + \beta_{12}s_1s_2 + \beta_{22}s_2^2.
\end{align*} 

At this point the provisional linear mean function is then fitted to the available data. There are many ways to do this, but the ordinary least squares method is typically used. This method yields an estimator $\hat\beta_{OLS}$ given by 

\begin{align*}
\hat\beta_{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} 
\end{align*} 

where $\mathbf{X} = [X(\mathbf{s}_1), X(\mathbf{s}_2), \dots, X(\mathbf{s}_n)]^T$ and $\mathbf{Y} = [Y(\mathbf{s}_1), Y(\mathbf{s}_2), \dots, Y(\mathbf{s}_n)]^T$. \footnote{Equivalently, and perhaps easier to work with, $\hat\beta_{OLS} = \text{argmin}\sum_{i=1}^n[Y(\mathbf{s}_i) - \mathbf{X}(\mathbf{s}_i)^T\mathbf{\beta}]^2$.} \\

It is possible to stop the analysis here, but once we have the second-order dependency structure of the semivariogram we can reestimate the mean function using estimated generalized least squares. A method given by Zimmerman and Stein (\cite{gelfand:2010}, p. 40) involves estimating a covariance matrix to include in the mean estimation. \footnote{Note to self: come back to this}

\section{Spatial Prediction: Kriging}
The goal of kriging is to find a predictor $\hat Y(\mathbf{s}_0)$ for $Y(\mathbf{s}_0)$ that minimizes the prediction error variance $\text{var}[\hat Y(\mathbf{s}_0) - Y(\mathbf{s}_0)$ of all possible predictors that are both (1), linear, and  (2) unbiased:

\begin{enumerate}
\item $\hat Y(\mathbf{s}_0) = \mathbf{\lambda}^T\mathbf{Y}$, where $\mathbf{\lambda}$ is a vector of fixed constants
\item $E[\hat Y(\mathbf{s}_0)] = E[Y(\mathbf{s}_0)]$, or equivalently, $\mathbf{\lambda}\mathbf{X} = \mathbf{X}(\mathbf{s}_0)$.
\end{enumerate}

If the true semivariogram is known, the universal kriging predictor is then given by

\begin{align*}
\hat Y(\mathbf{s}_0) = [\gamma + \mathbf{X}(\mathbf{X}^T\Gamma^{-1}\mathbf{X})^{-1}(\mathbf{x}_0 - \mathbf{X}^T\Gamma^{-1}\gamma)]^T\Gamma^{-1}\mathbf{Y}
\end{align*}

where $\gamma = [\gamma(\mathbf{s}_1 - \mathbf{s}_0), \dots, \gamma(\mathbf{s}_n - \mathbf{s}_0)]^T$, $\Gamma$ is the $n \times n$ symmetric matrix with $ij$th element $\gamma(\mathbf{s}_i - \mathbf{s}_j)$ and $\mathbf{x}_0 = \mathbf{X}(\mathbf{s}_0)$. \\

Minimizing the prediction error variance then gives us the kriging variance which can be expressed as

\begin{align*}
\sigma^2(\mathbf{s}_0) = \gamma^T\Gamma^{-1}\gamma - (\mathbf{X}^T\Gamma^{-1}\gamma - \mathbf{x}_0)^T(\mathbf{X}^T\Gamma^{-1}\mathbf{X})^{-1}(\mathbf{X}^T\Gamma^{-1}\gamma - \mathbf{x}_0).
\end{align*}

\newpage

\nocite{*}

\bibliographystyle{plain}

\bibliography{kriging}

\end{document}